keras layer

1. Convolutional layers

Conv1D 
This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to
    produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not 
    None, it is applied to the outputs as well.

keras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, 
    activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, 
    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)

------------------------------------------------------------------------------------------------------------------------------------------
Conv2D
This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, 
    a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well.

keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), 
    activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, 
    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)

------------------------------------------------------------------------------------------------------------------------------------------
Conv3D
This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, 
    a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well.

keras.layers.Conv3D(filters, kernel_size, strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1), 
    activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, 
    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)

------------------------------------------------------------------------------------------------------------------------------------------
SeparableConv1D
Separable convolutions consist in first performing a depthwise spatial convolution (which acts on each input channel separately) 
    followed by a pointwise convolution which mixes together the resulting output channels. The depth_multiplier argument controls 
    how many output channels are generated per input channel in the depthwise step.

keras.layers.SeparableConv1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, 
    depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform',
    pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, 
    bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None,     bias_constraint=None)

------------------------------------------------------------------------------------------------------------------------------------------
SeparableConv2D
Separable convolutions consist in first performing a depthwise spatial convolution (which acts on each input channel separately) 
    followed by a pointwise convolution which mixes together the resulting output channels. The depth_multiplier argument controls 
    how many output channels are generated per input channel in the depthwise step.

keras.layers.SeparableConv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), 
    depth_multiplier=1, activation=None, use_bias=True, depthwise_initializer='glorot_uniform', 
    pointwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, pointwise_regularizer=None, 
    bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, bias_constraint=None)

------------------------------------------------------------------------------------------------------------------------------------------
DepthwiseConv2D
Depthwise Separable convolutions consists in performing just the first step in a depthwise spatial convolution (which acts on each 
    input channel separately). The depth_multiplier argument controls how many output channels are generated per input channel 
    in the depthwise step.

keras.layers.DepthwiseConv2D(kernel_size, strides=(1, 1), padding='valid', depth_multiplier=1, data_format=None, 
    activation=None, use_bias=True, depthwise_initializer='glorot_uniform', bias_initializer='zeros', depthwise_regularizer=None, 
    bias_regularizer=None, activity_regularizer=None, depthwise_constraint=None, bias_constraint=None)

------------------------------------------------------------------------------------------------------------------------------------------
Conv2DTranspose
The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of
    a normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape
    of its input while maintaining a connectivity pattern that is compatible with said convolution.
When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include 
    the batch axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format="channels_last".

keras.layers.Conv2DTranspose(filters, kernel_size, strides=(1, 1), padding='valid', output_padding=None, data_format=None, 
    dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', 
    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)

------------------------------------------------------------------------------------------------------------------------------------------
Conv3DTranspose
The need for transposed convolutions generally arises from the desire to use a transformation going in the opposite direction of a 
    normal convolution, i.e., from something that has the shape of the output of some convolution to something that has the shape of 
    its input while maintaining a connectivity pattern that is compatible with said convolution.
When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include 
    the batch axis), e.g. input_shape=(128, 128, 128, 3) for a 128x128x128 volume with 3 channels if data_format="channels_last".

keras.layers.Conv3DTranspose(filters, kernel_size, strides=(1, 1, 1), padding='valid', output_padding=None, data_format=None, 
    activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, 
    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)

------------------------------------------------------------------------------------------------------------------------------------------
Cropping1D
Cropping layer for 1D input (e.g. temporal sequence). It crops along the time dimension (axis 1).

keras.layers.Cropping1D(cropping=(1, 1))

------------------------------------------------------------------------------------------------------------------------------------------
Cropping2D
Cropping layer for 2D input (e.g. picture). It crops along spatial dimensions, i.e. height and width.

keras.layers.Cropping2D(cropping=((0, 0), (0, 0)), data_format=None)

------------------------------------------------------------------------------------------------------------------------------------------
Cropping3D
Cropping layer for 3D data (e.g. spatial or spatio-temporal).

keras.layers.Cropping3D(cropping=((1, 1), (1, 1), (1, 1)), data_format=None)

------------------------------------------------------------------------------------------------------------------------------------------
UpSampling1D
Upsampling layer for 1D inputs. Repeats each temporal step size times along the time axis.

keras.layers.UpSampling1D(size=2)

------------------------------------------------------------------------------------------------------------------------------------------
UpSampling2D
Upsampling layer for 2D inputs. Repeats the rows and columns of the data by size[0] and size[1] respectively.

keras.layers.UpSampling2D(size=(2, 2), data_format=None, interpolation='nearest')

------------------------------------------------------------------------------------------------------------------------------------------
UpSampling3D
Upsampling layer for 3D inputs. Repeats the 1st, 2nd and 3rd dimensions of the data by size[0], size[1] and size[2] respectively.

keras.layers.UpSampling3D(size=(2, 2, 2), data_format=None)

------------------------------------------------------------------------------------------------------------------------------------------
ZeroPadding1D
Zero-padding layer for 1D input (e.g. temporal sequence).

keras.layers.ZeroPadding1D(padding=1)

------------------------------------------------------------------------------------------------------------------------------------------
ZeroPadding2D
Zero-padding layer for 2D input (e.g. picture). This layer can add rows and columns of zeros at the top, bottom, left and right side 
    of an image tensor.

keras.layers.ZeroPadding2D(padding=(1, 1), data_format=None)

------------------------------------------------------------------------------------------------------------------------------------------
ZeroPadding3D
Zero-padding layer for 3D data (spatial or spatio-temporal).

keras.layers.ZeroPadding3D(padding=(1, 1, 1), data_format=None)

------------------------------------------------------------------------------------------------------------------------------------------


2. Core layers

Dense
Regular densely-connected NN layer. Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the     element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a     bias vector created by the layer (only applicable if use_bias is True).

keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',             kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)

------------------------------------------------------------------------------------------------------------------------------------------
Activation
Applies an activation function to an output.

keras.layers.Activation(activation)

------------------------------------------------------------------------------------------------------------------------------------------
Dropout
Applies Dropout to the input. Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training     time, which helps prevent overfitting.

keras.layers.Dropout(rate, noise_shape=None, seed=None)

------------------------------------------------------------------------------------------------------------------------------------------
Flatten
Flattens the input. Does not affect the batch size.

keras.layers.Flatten(data_format=None)

------------------------------------------------------------------------------------------------------------------------------------------
Reshape
Reshapes an output to a certain shape.

keras.layers.Reshape(target_shape)

------------------------------------------------------------------------------------------------------------------------------------------
Permute
Permutes the dimensions of the input according to a given pattern. Useful for e.g. connecting RNNs and convnets together.

keras.layers.Permute(dims)

------------------------------------------------------------------------------------------------------------------------------------------
RepeatVector
Repeats the input n times.

keras.layers.RepeatVector(n)

------------------------------------------------------------------------------------------------------------------------------------------
Lambda
Wraps arbitrary expression as a Layer object.

keras.layers.Lambda(function, output_shape=None, mask=None, arguments=None)

------------------------------------------------------------------------------------------------------------------------------------------
ActivityRegularization
Layer that applies an update to the cost function based input activity.

keras.layers.ActivityRegularization(l1=0.0, l2=0.0)

------------------------------------------------------------------------------------------------------------------------------------------



3. Pooling layer

MaxPooling1D
Max pooling operation for temporal data.

keras.layers.MaxPooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last')

------------------------------------------------------------------------------------------------------------------------------------------
MaxPooling2D
Max pooling operation for spatial data.

keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)

------------------------------------------------------------------------------------------------------------------------------------------
MaxPooling3D
Max pooling operation for 3D data (spatial or spatio-temporal).

keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None)

------------------------------------------------------------------------------------------------------------------------------------------
AveragePooling1D
Average pooling for temporal data.

keras.layers.AveragePooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last')

------------------------------------------------------------------------------------------------------------------------------------------
AveragePooling2D
Average pooling operation for spatial data.

keras.layers.AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)

------------------------------------------------------------------------------------------------------------------------------------------
AveragePooling3D
Average pooling operation for 3D data (spatial or spatio-temporal).

keras.layers.AveragePooling3D(pool_size=(2, 2, 2), strides=None, padding='valid', data_format=None)

------------------------------------------------------------------------------------------------------------------------------------------
GlobalMaxPooling1D
Global max pooling operation for temporal data.

keras.layers.GlobalMaxPooling1D(data_format='channels_last')

------------------------------------------------------------------------------------------------------------------------------------------
GlobalAveragePooling1D
Global average pooling operation for temporal data.

keras.layers.GlobalAveragePooling1D(data_format='channels_last')

------------------------------------------------------------------------------------------------------------------------------------------
GlobalMaxPooling2D
Global max pooling operation for spatial data.

keras.layers.GlobalMaxPooling2D(data_format=None)

------------------------------------------------------------------------------------------------------------------------------------------
GlobalAveragePooling2D
Global average pooling operation for spatial data.

keras.layers.GlobalAveragePooling2D(data_format=None)

------------------------------------------------------------------------------------------------------------------------------------------
GlobalMaxPooling3D
Global Max pooling operation for 3D data.

keras.layers.GlobalMaxPooling3D(data_format=None)

------------------------------------------------------------------------------------------------------------------------------------------
GlobalAveragePooling3D
Global Average pooling operation for 3D data.

keras.layers.GlobalAveragePooling3D(data_format=None)

------------------------------------------------------------------------------------------------------------------------------------------


4. Recurrent layer

RNN
keras.layers.RNN(cell, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)

------------------------------------------------------------------------------------------------------------------------------------------
SimpleRNN
Fully-connected RNN where the output is to be fed back to input.

keras.layers.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform',         recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None,         bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,         dropout=0.0, recurrent_dropout=0.0, return_sequences=False, return_state=False, go_backwards=False, stateful=False,     unroll=False) 

------------------------------------------------------------------------------------------------------------------------------------------
GRU
Gated Recurrent Unit

keras.layers.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform',         recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None,         bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,         dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False,         stateful=False, unroll=False, reset_after=False)

------------------------------------------------------------------------------------------------------------------------------------------
LSTM
Long Short-Term Memory layer 

keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True,     kernel_initializer='glorot_uniform',     recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True,     kernel_regularizer=None,     recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,     kernel_constraint=None, recurrent_constraint=None,     bias_constraint=None, dropout=0.0, recurrent_dropout=0.0,implementation=1, return_sequences=False, return_state=False,         go_backwards=False, stateful=False, unroll=False)

------------------------------------------------------------------------------------------------------------------------------------------
ConvLSTM2D
Convolutional LSTM

keras.layers.ConvLSTM2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1),         activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform',         recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None,         recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,     recurrent_constraint=None,     bias_constraint=None, return_sequences=False, go_backwards=False, stateful=False, dropout=0.0,     recurrent_dropout=0.0)

------------------------------------------------------------------------------------------------------------------------------------------
ConvLSTM2DCell
Cell class for the ConvLSTM2D layer.

keras.layers.ConvLSTM2DCell(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1),     activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform',     recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None,     recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,     dropout=0.0, recurrent_dropout=0.0)

------------------------------------------------------------------------------------------------------------------------------------------
SimpleRNNCell
Cell class for SimpleRNN.

keras.layers.SimpleRNNCell(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform',             recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None,             bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0,              recurrent_dropout=0.0)

------------------------------------------------------------------------------------------------------------------------------------------
GRUCell
Cell class for the GRU layer

keras.layers.GRUCell(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True,     kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None,     recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,     dropout=0.0, recurrent_dropout=0.0, implementation=1, reset_after=False)

------------------------------------------------------------------------------------------------------------------------------------------
LSTMCell
Cell class for the LSTM layer.

keras.layers.LSTMCell(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True,     kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True,     kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, kernel_constraint=None,     recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1)

------------------------------------------------------------------------------------------------------------------------------------------
CuDNNGRU
Fast GRU implementation backed by CuDNN.

keras.layers.CuDNNGRU(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros',     kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,     kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False, return_state=False,     stateful=False)

------------------------------------------------------------------------------------------------------------------------------------------
CuDNNLSTM
Fast LSTM implementation with CuDNN.

keras.layers.CuDNNLSTM(units, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros',     unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None,     activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, return_sequences=False,     return_state=False, stateful=False)

------------------------------------------------------------------------------------------------------------------------------------------


5. Embedding layer

Embedding
Turns positive integers (indexes) into dense vectors of fixed size.

keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None,     activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)

------------------------------------------------------------------------------------------------------------------------------------------


6. Merge layers

Add
Layer that adds a list of inputs.

keras.layers.Add()

------------------------------------------------------------------------------------------------------------------------------------------
Subtract
Layer that subtracts two inputs.

keras.layers.Subtract()

------------------------------------------------------------------------------------------------------------------------------------------
Multiply
Layer that multiplies (element-wise) a list of inputs.

keras.layers.Multiply()

------------------------------------------------------------------------------------------------------------------------------------------
Average
Layer that averages a list of inputs.

keras.layers.Average()

------------------------------------------------------------------------------------------------------------------------------------------
Maximum
Layer that computes the maximum (element-wise) a list of inputs.

keras.layers.Maximum()

------------------------------------------------------------------------------------------------------------------------------------------
Minimum
Layer that computes the minimum (element-wise) a list of inputs.

keras.layers.Minimum()

------------------------------------------------------------------------------------------------------------------------------------------
Concatenate
Layer that concatenates a list of inputs.

keras.layers.Concatenate(axis=-1)

------------------------------------------------------------------------------------------------------------------------------------------
Dot
Layer that computes a dot product between samples in two tensors.

keras.layers.Dot(axes, normalize=False)

------------------------------------------------------------------------------------------------------------------------------------------


7.Advanced Activations Layers


LeakyReLU
Leaky version of a Rectified Linear Unit.It allows a small gradient when the unit is not active: f(x) = alpha * x for x < 0, f(x) = x for     x >= 0.

keras.layers.LeakyReLU(alpha=0.3)

------------------------------------------------------------------------------------------------------------------------------------------
PReLU
Parametric Rectified Linear Unit. It follows: f(x) = alpha * x for x < 0, f(x) = x for x >= 0, where alpha is a learned array with the     same shape as x.

keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=None)

------------------------------------------------------------------------------------------------------------------------------------------
ELU
Exponential Linear Unit. It follows: f(x) =  alpha * (exp(x) - 1.) for x < 0, f(x) = x for x >= 0.

keras.layers.ELU(alpha=1.0)

------------------------------------------------------------------------------------------------------------------------------------------
ThresholdedReLU
Thresholded Rectified Linear Unit. It follows: f(x) = x for x > theta, f(x) = 0 otherwise.

keras.layers.ThresholdedReLU(theta=1.0)

------------------------------------------------------------------------------------------------------------------------------------------
Softmax

keras.layers.Softmax(axis=-1)

------------------------------------------------------------------------------------------------------------------------------------------
ReLU
Rectified Linear Unit activation function. With default values, it returns element-wise max(x, 0).

keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)

------------------------------------------------------------------------------------------------------------------------------------------



8. Normalization Layers

BatchNormalization
Batch normalization layer (Ioffe and Szegedy, 2014). Normalize the activations of the previous layer at each batch, i.e. applies a     transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.

keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)

------------------------------------------------------------------------------------------------------------------------------------------



























Usage of optimizers


1. SGD
Stochastic gradient descent optimizer. Includes support for momentum, learning rate decay, and Nesterov momentum.

keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)

------------------------------------------------------------------------------------------------------------------------------------------
2. RMSprop
Recommended to leave the parameters of this optimizer at their default values (except the learning rate, which can be freely tuned). This optimizer is usually a good choice for recurrent neural networks.

keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)

------------------------------------------------------------------------------------------------------------------------------------------
3. Adagrad
Adagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training. The more updates a parameter receives, the smaller the learning rate.It is recommended to leave the parameters of this optimizer at their default values.

keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)

------------------------------------------------------------------------------------------------------------------------------------------
4. AdadeltaAdamax
Adadelta is a more robust extension of Adagrad that adapts learning rates based on a moving window of gradient updates, instead of accumulating all past gradients. 

keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)

------------------------------------------------------------------------------------------------------------------------------------------
5. Adam
Default parameters follow those provided in the original paper.

keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)

------------------------------------------------------------------------------------------------------------------------------------------
6. Adamax
It is a variant of Adam based on the infinity norm. Default parameters follow those provided in the paper.

keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)

------------------------------------------------------------------------------------------------------------------------------------------
7. Nadam
Much like Adam is essentially RMSprop with momentum, Nadam is Adam RMSprop with Nesterov momentum.

keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)

------------------------------------------------------------------------------------------------------------------------------------------































Usage of activations


1. softmax

keras.activations.softmax(x, axis=-1)

------------------------------------------------------------------------------------------------------------------------------------------
2. elu
Exponential linear unit.

keras.activations.elu(x, alpha=1.0)

------------------------------------------------------------------------------------------------------------------------------------------
3. selu
Scaled Exponential Linear Unit (SELU).

keras.activations.selu(x)

------------------------------------------------------------------------------------------------------------------------------------------
4. softplus

keras.activations.softplus(x)

------------------------------------------------------------------------------------------------------------------------------------------
5. softsign

keras.activations.softsign(x)

------------------------------------------------------------------------------------------------------------------------------------------
6. relu
Rectified Linear Unit. With default values, it returns element-wise max(x, 0).

keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0)

------------------------------------------------------------------------------------------------------------------------------------------
7. tanh
Hyperbolic tangent activation function.

keras.activations.tanh(x)

------------------------------------------------------------------------------------------------------------------------------------------
8. sigmoid

keras.activations.sigmoid(x)

------------------------------------------------------------------------------------------------------------------------------------------
9. hard_sigmoid
Faster to compute than sigmoid activation.

keras.activations.hard_sigmoid(x)

------------------------------------------------------------------------------------------------------------------------------------------
10. exponential

keras.activations.exponential(x)

------------------------------------------------------------------------------------------------------------------------------------------
11. linear

keras.activations.linear(x)

------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------



------------------------------------------------------------------------------------------------------------------------------------------
